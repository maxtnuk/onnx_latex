// vim: ft=arm

// C tile regs: v16 to v31, no need to preserve
// 
//      v16[0] v18[0] v20[0] v22[0] v24[0] v26[0] v28[0] v30[0]
//      v16[1] v18[1] 
//      v16[2] v18[2] 
//      v16[3] v18[3]
//                     
//      v17[0] v19[0] v21[0] v23[0] v25[0] v27[0] v29[0] v31[0]
//      v17[1] v19[1] 
//      v17[2] v19[2] 
//      v17[3] v19[3] 

// no preservation either for v0-v7...
// v8..v15 are callee-preserved
// packed A buffering (2x8 values): alternating v0, v1 with v2, v3
// packed B buffering (2x8 values): alternating v4, v5 with v6, v7

.text
.align 4

.cpu generic+fp+simd
.global {{G}}arm64simd_mmm_f32_64x1_{{core}}
{{G}}arm64simd_mmm_f32_64x1_{{core}}:

/*
    prfm        pldl1keep, [x1]
    prfm        pldl1keep, [x2]
*/

    sub	        sp, sp, 64
    stp         d8, d9, [sp, #0]
    stp         d10, d11, [sp, #16]
    stp         d12, d13, [sp, #32]
    stp         d14, d15, [sp, #48]

{% for r in (16..31) %}
    eor         v{{r}}.8b, v{{r}}.8b, v{{r}}.8b
{% endfor %}

    ldp         x7, x8, [x0]        // a, b
    ldp         x9, x10, [x0, #16]  // c, lin

    ldp         x2, x1, [x7]        // a disc, a first arg

    cmp         x2, #1
    bne         .unsupported

    ldp         x5, x3, [x10]       // lin disc, k
    cmp         x5, #0
    bne         .unsupported
    cmp         x3, #0
    beq         .non_linear

    ldp         x4, x2, [x8]        // b disc, first arg
    cmp         x4, #1
    beq         .packed_packed
    cmp         x4, #2
    beq         .packed_tops_and_offsets
    cmp         x4, #3
    beq         .packed_vec_strides
    b           .unsupported

.packed_tops_and_offsets:
    ldr         x8, [x8, #16]       // cols ptr ptr (x2 = row offsets ptr)
    ldr         x4, [ x2 ], #8      // fist row offset

    ldr         x5, [x8]            // heads of cols ptr

    cmp         x3, #4
    blt         .packed_tops_and_offsets_loop_1

    cmp x3, #0
    beq .non_linear

.p2align 4
.packed_tops_and_offsets_loop_1:
    add         x8, x4, x5
    ld1         {v8.s}[0], [ x8 ]

    ld1         { v0.4s, v1.4s, v2.4s, v3.4s }, [ x1 ], #64
    ld1         { v4.4s, v5.4s, v6.4s, v7.4s }, [ x1 ], #64

    fmla        v16.4s, v0.4s, v8.s[0]
    fmla        v17.4s, v1.4s, v8.s[0]
    fmla        v18.4s, v2.4s, v8.s[0]
    fmla        v19.4s, v3.4s, v8.s[0]
    ld1         { v0.4s, v1.4s, v2.4s, v3.4s }, [ x1 ], #64

    fmla        v20.4s, v4.4s, v8.s[0]
    fmla        v21.4s, v5.4s, v8.s[0]
    fmla        v22.4s, v6.4s, v8.s[0]
    fmla        v23.4s, v7.4s, v8.s[0]

    ld1         { v4.4s, v5.4s, v6.4s, v7.4s }, [ x1 ], #64
    fmla        v24.4s, v0.4s, v8.s[0]
    fmla        v25.4s, v1.4s, v8.s[0]
    fmla        v26.4s, v2.4s, v8.s[0]
    fmla        v27.4s, v3.4s, v8.s[0]
    ldr         x4, [ x2 ], #8
    fmla        v28.4s, v4.4s, v8.s[0]
    fmla        v29.4s, v5.4s, v8.s[0]
    fmla        v30.4s, v6.4s, v8.s[0]
    fmla        v31.4s, v7.4s, v8.s[0]

    subs        x3, x3, #1
    bne         .packed_tops_and_offsets_loop_1

    b           .non_linear

.packed_packed:
    mov         x4, #4
    b           .packed_vec_strides_start


.packed_vec_strides:
    // x2 ->  b ptr
    ldr         x4, [x8, #16]    // b stride

.packed_vec_strides_start:

/*

    cmp		x3, #4
    blt		.packed_vec_strides_loop_1

.p2align 4
.packed_vec_strides_loop_4:

    ld1         { v8.s }[0], [ x2 ], x4
    ld1         { v0.4s, v1.4s, v2.4s, v3.4s }, [ x1 ], #64
    ld1         { v4.4s, v5.4s, v6.4s, v7.4s }, [ x1 ], #64
    ld1         { v8.s }[0], [ x2 ], x4
    ld1         { v10.s }[0], [ x2 ], x4
    ld1         { v11.s }[0], [ x2 ], x4

    fmla        v16.4s, v0.4s, v8.s[0]
    fmla        v17.4s, v1.4s, v8.s[0]
    fmla        v18.4s, v2.4s, v8.s[0]
    fmla        v19.4s, v3.4s, v8.s[0]
    ld1         { v0.4s, v1.4s, v2.4s, v3.4s }, [ x1 ], #64

    fmla        v20.4s, v4.4s, v8.s[0]
    fmla        v21.4s, v5.4s, v8.s[0]
    fmla        v22.4s, v6.4s, v8.s[0]
    fmla        v23.4s, v7.4s, v8.s[0]
    ld1         { v4.4s, v5.4s, v6.4s, v7.4s }, [ x1 ], #64

    fmla        v24.4s, v0.4s, v8.s[0]
    fmla        v25.4s, v1.4s, v8.s[0]
    fmla        v26.4s, v2.4s, v8.s[0]
    fmla        v27.4s, v3.4s, v8.s[0]
    ld1         { v0.4s, v1.4s, v2.4s, v3.4s }, [ x1 ], #64

    fmla        v28.4s, v4.4s, v8.s[0]
    fmla        v29.4s, v5.4s, v8.s[0]
    fmla        v30.4s, v6.4s, v8.s[0]
    fmla        v31.4s, v7.4s, v8.s[0]
    ld1         { v4.4s, v5.4s, v6.4s, v7.4s }, [ x1 ], #64

    fmla        v16.4s, v0.4s, v8.s[0]
    fmla        v17.4s, v1.4s, v8.s[0]
    fmla        v18.4s, v2.4s, v8.s[0]
    fmla        v19.4s, v3.4s, v8.s[0]
    ld1         { v0.4s, v1.4s, v2.4s, v3.4s }, [ x1 ], #64

    fmla        v20.4s, v4.4s, v8.s[0]
    fmla        v21.4s, v5.4s, v8.s[0]
    fmla        v22.4s, v6.4s, v8.s[0]
    fmla        v23.4s, v7.4s, v8.s[0]
    ld1         { v4.4s, v5.4s, v6.4s, v7.4s }, [ x1 ], #64

    fmla        v24.4s, v0.4s, v8.s[0]
    fmla        v25.4s, v1.4s, v8.s[0]
    fmla        v26.4s, v2.4s, v8.s[0]
    fmla        v27.4s, v3.4s, v8.s[0]
    ld1         { v0.4s, v1.4s, v2.4s, v3.4s }, [ x1 ], #64

    fmla        v28.4s, v4.4s, v8.s[0]
    fmla        v29.4s, v5.4s, v8.s[0]
    fmla        v30.4s, v6.4s, v8.s[0]
    fmla        v31.4s, v7.4s, v8.s[0]
    ld1         { v4.4s, v5.4s, v6.4s, v7.4s }, [ x1 ], #64

    fmla        v16.4s, v0.4s, v10.s[0]
    fmla        v17.4s, v1.4s, v10.s[0]
    fmla        v18.4s, v2.4s, v10.s[0]
    fmla        v19.4s, v3.4s, v10.s[0]
    ld1         { v0.4s, v1.4s, v2.4s, v3.4s }, [ x1 ], #64

    fmla        v20.4s, v4.4s, v10.s[0]
    fmla        v21.4s, v5.4s, v10.s[0]
    fmla        v22.4s, v6.4s, v10.s[0]
    fmla        v23.4s, v7.4s, v10.s[0]
    ld1         { v4.4s, v5.4s, v6.4s, v7.4s }, [ x1 ], #64

    fmla        v24.4s, v0.4s, v10.s[0]
    fmla        v25.4s, v1.4s, v10.s[0]
    fmla        v26.4s, v2.4s, v10.s[0]
    fmla        v27.4s, v3.4s, v10.s[0]
    ld1         { v0.4s, v1.4s, v2.4s, v3.4s }, [ x1 ], #64

    fmla        v28.4s, v4.4s, v10.s[0]
    fmla        v29.4s, v5.4s, v10.s[0]
    fmla        v30.4s, v6.4s, v10.s[0]
    fmla        v31.4s, v7.4s, v10.s[0]
    ld1         { v4.4s, v5.4s, v6.4s, v7.4s }, [ x1 ], #64

    fmla        v16.4s, v0.4s, v11.s[0]
    fmla        v17.4s, v1.4s, v11.s[0]
    fmla        v18.4s, v2.4s, v11.s[0]
    fmla        v19.4s, v3.4s, v11.s[0]
    ld1         { v0.4s, v1.4s, v2.4s, v3.4s }, [ x1 ], #64

    fmla        v20.4s, v4.4s, v11.s[0]
    fmla        v21.4s, v5.4s, v11.s[0]
    fmla        v22.4s, v6.4s, v11.s[0]
    fmla        v23.4s, v7.4s, v11.s[0]
    ld1         { v4.4s, v5.4s, v6.4s, v7.4s }, [ x1 ], #64

    fmla        v24.4s, v0.4s, v11.s[0]
    fmla        v25.4s, v1.4s, v11.s[0]
    fmla        v26.4s, v2.4s, v11.s[0]
    fmla        v27.4s, v3.4s, v11.s[0]

    fmla        v28.4s, v4.4s, v11.s[0]
    fmla        v29.4s, v5.4s, v11.s[0]
    fmla        v30.4s, v6.4s, v11.s[0]
    fmla        v31.4s, v7.4s, v11.s[0]

    sub 	x3, x3, #4
    cmp		x3, #4
    bge		.packed_vec_strides_loop_4

    cmp 	x3, #0
    beq		.non_linear
    */

.p2align 4
.packed_vec_strides_loop_1:

    ld1         { v8.s }[0], [ x2 ], x4
    ld1         { v0.4s, v1.4s, v2.4s, v3.4s }, [ x1 ], #64
    ld1         { v4.4s, v5.4s, v6.4s, v7.4s }, [ x1 ], #64

    fmla        v16.4s, v0.4s, v8.s[0]
    fmla        v17.4s, v1.4s, v8.s[0]
    fmla        v18.4s, v2.4s, v8.s[0]
    fmla        v19.4s, v3.4s, v8.s[0]
    ld1         { v0.4s, v1.4s, v2.4s, v3.4s }, [ x1 ], #64

    fmla        v20.4s, v4.4s, v8.s[0]
    fmla        v21.4s, v5.4s, v8.s[0]
    fmla        v22.4s, v6.4s, v8.s[0]
    fmla        v23.4s, v7.4s, v8.s[0]

    ld1         { v4.4s, v5.4s, v6.4s, v7.4s }, [ x1 ], #64
    fmla        v24.4s, v0.4s, v8.s[0]
    fmla        v25.4s, v1.4s, v8.s[0]
    fmla        v26.4s, v2.4s, v8.s[0]
    fmla        v27.4s, v3.4s, v8.s[0]
    fmla        v28.4s, v4.4s, v8.s[0]
    fmla        v29.4s, v5.4s, v8.s[0]
    fmla        v30.4s, v6.4s, v8.s[0]
    fmla        v31.4s, v7.4s, v8.s[0]

    subs        x3, x3, #1
    bne         .packed_vec_strides_loop_1

.non_linear:
    ldr         x1, [x0, #32]
    cmp         x1, #0
    bne         .non_linear_loop_entry

.store:
    ldr         x3, [x0, #16]               // c
    ldr         x4, [x3]                    // c disc
    cmp         x4, #0
    beq         .store_strides
    cmp         x4, #3
    beq         .store_strides
    b           .unsupported

.store_strides:
    ldp         x5, x6, [x3, #8]                // c base ptr
    mov         x0, #0

    cmp         x6, #4
    beq         .store_strides_contig

    {% for reg in (16..31) %}
        {% for lane in (0..3) %}
            st1 { v{{reg}}.s }[{{lane}}], [ x5 ], x6
        {% endfor %}
    {% endfor %}
    b           .return

.store_strides_contig:

    {% for reg in (16..31) %}
        st1 { v{{reg}}.4s }, [ x5 ], #16
    {% endfor %}

.return:

    ldp         d8, d9, [sp, #0]
    ldp         d10, d11, [sp, #16]
    ldp         d12, d13, [sp, #32]
    ldp         d14, d15, [sp, #48]
    add         sp, sp, 64

    ret

.non_linear_loop_entry:
    sub         x1, x1, 24

.non_linear_loop:
    add         x1, x1, 24
    ldr         x2, [x1]
    cmp         x2, #0
    beq         .store
    cmp         x2, #1
    beq         .min
    cmp         x2, #2
    beq         .max
    cmp         x2, #3
    beq         .non_linear_addc
    cmp         x2, #4
    beq         .per_row_mul
    cmp         x2, #5
    beq         .per_row_add
    cmp         x2, #6
    beq         .per_col_mul
    cmp         x2, #7
    beq         .per_col_add
    cmp         x2, #8
    beq         .add_row_col_product
    cmp         x2, #9
    beq         .scalar_mul
    cmp         x2, #10
    beq         .scalar_add

    add         x0, x2, #4000
    b           .return

.min:
    add         x2, x1, #8
    ld1         {v0.s}[0], [ x2 ]
    dup         v0.4s, v0.s[0]
    {% for reg in (16..31) %}
        fmin        v{{reg}}.4s, v{{reg}}.4s, v0.4s
    {% endfor %}

    b           .non_linear_loop

.max:
    add         x2, x1, #8
    ld1         {v0.s}[0], [ x2 ]
    dup         v0.4s, v0.s[0]
    {% for reg in (16..31) %}
        fmax        v{{reg}}.4s, v{{reg}}.4s, v0.4s
    {% endfor %}

    b           .non_linear_loop

.non_linear_addc:
    ldr         x3, [x0, #16]               // c
    ldr         x4, [x3]                    // c disc
    cmp         x4, #0
    bne         .unsupported

    ldr         x5, [x3, #8]                // c base ptr
    ldr         x6, [x3, #16]               // rsc

    {% for reg in (16..31) %}
        {% for lane in (0..3) %}
            ld1 {v0.s}[{{lane}}], [ x5 ], x6
        {% endfor %}
        fadd v{{reg}}.4s, v{{reg}}.4s, v0.4s
    {% endfor %}

    b           .non_linear_loop

.per_row_mul:
    ldr         x2, [x1, #8]

    {% for r in (0..15) %}
        ldr     q{{r}}, [x2], #16
    {% endfor %}

    {% for r in (0..15) %}
        fmul v{{r| plus: 16}}.4s, v{{r | plus: 16}}.4s, v{{r}}.4s
    {% endfor %}

    b           .non_linear_loop

.per_row_add:
    ldr         x2, [x1, #8]

    {% for r in (0..15) %}
        ldr     q{{r}}, [x2], #16
    {% endfor %}

    {% for r in (0..15) %}
        fadd v{{r| plus: 16}}.4s, v{{r | plus: 16}}.4s, v{{r}}.4s
    {% endfor %}

    b           .non_linear_loop

.add_row_col_product:
    ldr     x3, [x1, #16]
    ldr     x2, [x1, #8]

    ld1         {v8.s}[0], [ x3 ]

    {% for r in (0..7) %}
        ldr     q{{r}}, [x2], #16
    {% endfor %}

    fmla        v16.4s, v0.4s, v8.s[0]
    ldr         q0, [x2], #16
    fmla        v17.4s, v1.4s, v8.s[0] 
    ldr         q1, [x2], #16
    fmla        v18.4s, v2.4s, v8.s[0] 
    ldr         q2, [x2], #16
    fmla        v19.4s, v3.4s, v8.s[0] 
    ldr         q3, [x2], #16
    fmla        v20.4s, v4.4s, v8.s[0] 
    ldr         q4, [x2], #16
    fmla        v21.4s, v5.4s, v8.s[0] 
    ldr         q5, [x2], #16
    fmla        v22.4s, v6.4s, v8.s[0] 
    ldr         q6, [x2], #16
    fmla        v23.4s, v7.4s, v8.s[0] 
    ldr         q7, [x2], #16

    fmla        v24.4s, v0.4s, v8.s[0]
    fmla        v25.4s, v1.4s, v8.s[0] 
    fmla        v26.4s, v2.4s, v8.s[0] 
    fmla        v27.4s, v3.4s, v8.s[0] 
    fmla        v28.4s, v4.4s, v8.s[0] 
    fmla        v29.4s, v5.4s, v8.s[0] 
    fmla        v30.4s, v6.4s, v8.s[0] 
    fmla        v31.4s, v7.4s, v8.s[0] 

    b           .non_linear_loop

.scalar_mul:
    add         x2, x1, #8
    b           .mul_all
.per_col_mul:
    ldr         x2, [x1, #8]
.mul_all:
    ld1         {v0.s}[0], [ x2 ]
    dup         v0.4s, v0.s[0]
    {% for reg in (16..31) %}
        fmul        v{{reg}}.4s, v{{reg}}.4s, v0.4s
    {% endfor %}

    b           .non_linear_loop

.scalar_add:
    add         x2, x1, #8
    b           .add_all
.per_col_add:
    ldr         x2, [x1, #8]
.add_all:
    ld1         {v0.s}[0], [ x2 ]
    dup         v0.4s, v0.s[0]
    {% for reg in (16..31) %}
        fadd        v{{reg}}.4s, v{{reg}}.4s, v0.4s
    {% endfor %}

    b           .non_linear_loop

.unsupported:
    mov         x0, #1
    b           .return
